{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from typing import List, Tuple\n",
    "from sklearn.metrics import f1_score\n",
    "from torch import Tensor, tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from dao.ower.ower_dir import OwerDir\n",
    "from dao.ower.samples_tsv import Sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ower_dir_path = 'data/ower/ower-v4-fb-irt-100-5/'\n",
    "class_count = 100\n",
    "sent_count = 5\n",
    "\n",
    "batch_size = 64\n",
    "sent_len = 64\n",
    "\n",
    "epoch_count = 20\n",
    "lr = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ower_dir = OwerDir(Path(ower_dir_path))\n",
    "ower_dir.check()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "bert = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=class_count)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set = ower_dir.train_samples_tsv.load(class_count, sent_count)\n",
    "valid_set = ower_dir.valid_samples_tsv.load(class_count, sent_count)\n",
    "\n",
    "\n",
    "def generate_batch(batch: List[Sample]) -> Tuple[Tensor, Tensor]:\n",
    "\n",
    "    _, _, classes_batch, sents_batch = zip(*batch)\n",
    "\n",
    "    for sents in sents_batch:\n",
    "        shuffle(sents)\n",
    "\n",
    "    contexts_batch = [' '.join(sents) for sents in sents_batch]\n",
    "\n",
    "    encoded_batch = tokenizer(contexts_batch, padding=True, truncation=True, max_length=sent_len, return_tensors='pt')\n",
    "\n",
    "    return encoded_batch, tensor(classes_batch)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=generate_batch, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=generate_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = BCEWithLogitsLoss()\n",
    "optimizer = Adam(bert.parameters(), lr=lr)\n",
    "\n",
    "bert = bert.cuda()\n",
    "\n",
    "for epoch in range(epoch_count):\n",
    "\n",
    "    bert.train()\n",
    "    for step, (ctxt_batch, gt_batch) in enumerate(tqdm(train_loader)):\n",
    "        input_ids = ctxt_batch.input_ids.cuda()\n",
    "        attention_mask = ctxt_batch.attention_mask.cuda()\n",
    "        gt_batch = gt_batch.cuda()\n",
    "\n",
    "        pred_batch = bert(input_ids, attention_mask).logits\n",
    "\n",
    "        loss = criterion(gt_batch.float(), pred_batch)\n",
    "        print(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    bert.eval()\n",
    "    for step, (ctxt_batch, gt_batch) in enumerate(tqdm(valid_loader)):\n",
    "        input_ids = ctxt_batch.input_ids.cuda()\n",
    "        attention_mask = ctxt_batch.attention_mask.cuda()\n",
    "        gt_batch = gt_batch.cuda()\n",
    "\n",
    "        pred_batch = bert(input_ids, attention_mask).logits > 0\n",
    "\n",
    "        f1 = f1_score(gt_batch.detach().cpu().numpy(),\n",
    "                      pred_batch.detach().cpu().numpy(),\n",
    "                      average=None)\n",
    "\n",
    "        print('### f1 = ', f1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}